
The following have been reloaded with a version change:
  1) cuda/11.8.0-fasrc01 => cuda/12.4.1-fasrc01
  2) cudnn/8.9.2.26_cuda11-fasrc01 => cudnn/9.1.1.17_cuda12-fasrc01

The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[W1214 23:36:31.227348232 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:51,936 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:53,971 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:54,033 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
2024-12-14 23:37:54,035 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:54,551 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:55,539 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:55,717 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
2024-12-14 23:37:55,783 [INFO] Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 526398464 bytes required
  - 1: 513802240 bytes required
  - 2: 513802240 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
[W1214 23:37:57.631471153 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W1214 23:37:58.515114070 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W1214 23:37:58.058863599 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W1214 23:38:00.525303112 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
2024-12-14 23:38:02,406 [WARNING] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

Starting epoch 1/10...
Starting epoch 1/10...

Starting epoch 1/10...

Starting epoch 1/10...

[rank2]: Traceback (most recent call last):
[rank2]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/main.py", line 195, in <module>
[rank2]:     epoch_losses = train_learner_with_target(learner, drafters, target_model, data_loader, 
[rank2]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/models/training.py", line 49, in train_learner_with_target
[rank2]:     for step, (input_ids, features) in enumerate(data_loader):
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/data_loader.py", line 552, in __iter__
[rank2]:     current_batch = next(dataloader_iter)
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank2]:     data = self._next_data()
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
[rank2]:     return self._process_data(data)
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
[rank2]:     data.reraise()
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
[rank2]:     raise exception
[rank2]: RuntimeError: Caught RuntimeError in DataLoader worker process 0.
[rank2]: Original Traceback (most recent call last):
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
[rank2]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank2]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank2]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/datasetutils.py", line 29, in __getitem__
[rank2]:     input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).cuda()
[rank2]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 305, in _lazy_init
[rank2]:     raise RuntimeError(
[rank2]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

[rank3]: Traceback (most recent call last):
[rank3]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/main.py", line 195, in <module>
[rank3]:     epoch_losses = train_learner_with_target(learner, drafters, target_model, data_loader, 
[rank3]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/models/training.py", line 49, in train_learner_with_target
[rank3]:     for step, (input_ids, features) in enumerate(data_loader):
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/data_loader.py", line 552, in __iter__
[rank3]:     current_batch = next(dataloader_iter)
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank3]:     data = self._next_data()
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
[rank3]:     return self._process_data(data)
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
[rank3]:     data.reraise()
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
[rank3]:     raise exception
[rank3]: RuntimeError: Caught RuntimeError in DataLoader worker process 0.
[rank3]: Original Traceback (most recent call last):
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
[rank3]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank3]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank3]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/datasetutils.py", line 29, in __getitem__
[rank3]:     input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).cuda()
[rank3]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 305, in _lazy_init
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

[rank0]: Traceback (most recent call last):
[rank0]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/main.py", line 195, in <module>
[rank0]:     epoch_losses = train_learner_with_target(learner, drafters, target_model, data_loader, 
[rank0]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/models/training.py", line 49, in train_learner_with_target
[rank0]:     for step, (input_ids, features) in enumerate(data_loader):
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/data_loader.py", line 552, in __iter__
[rank0]:     current_batch = next(dataloader_iter)
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank0]:     data = self._next_data()
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
[rank0]:     return self._process_data(data)
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
[rank0]:     raise exception
[rank0]: RuntimeError: Caught RuntimeError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/datasetutils.py", line 29, in __getitem__
[rank0]:     input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).cuda()
[rank0]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 305, in _lazy_init
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

[rank1]: Traceback (most recent call last):
[rank1]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/main.py", line 195, in <module>
[rank1]:     epoch_losses = train_learner_with_target(learner, drafters, target_model, data_loader, 
[rank1]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/models/training.py", line 49, in train_learner_with_target
[rank1]:     for step, (input_ids, features) in enumerate(data_loader):
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/data_loader.py", line 552, in __iter__
[rank1]:     current_batch = next(dataloader_iter)
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank1]:     data = self._next_data()
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
[rank1]:     return self._process_data(data)
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
[rank1]:     data.reraise()
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/_utils.py", line 715, in reraise
[rank1]:     raise exception
[rank1]: RuntimeError: Caught RuntimeError in DataLoader worker process 0.
[rank1]: Original Traceback (most recent call last):
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
[rank1]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank1]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank1]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/n/holylfs06/LABS/kempner_fellow_emalach/Lab/rli/2281-Project/datasetutils.py", line 29, in __getitem__
[rank1]:     input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).cuda()
[rank1]:   File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/cuda/__init__.py", line 305, in _lazy_init
[rank1]:     raise RuntimeError(
[rank1]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

[rank0]:[W1214 23:38:37.332081347 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1214 23:38:41.583895 959932 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 959953 closing signal SIGTERM
W1214 23:38:41.591463 959932 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 959954 closing signal SIGTERM
W1214 23:38:41.591632 959932 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 959955 closing signal SIGTERM
E1214 23:38:42.871246 959932 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 959956) of binary: /n/home13/rli42/conda/envs/2281-project-env/bin/python3.9
Traceback (most recent call last):
  File "/n/home13/rli42/conda/envs/2281-project-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/n/home13/rli42/conda/envs/2281-project-env/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-14_23:38:41
  host      : holygpu8a13402.rc.fas.harvard.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 959956)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
